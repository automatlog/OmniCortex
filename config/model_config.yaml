llm:
  provider: "vllm"
  default_model: "meta-llama/Llama-3.1-8B-Instruct"
  temperature: 0.6
  max_tokens: 2048
  timeout: 30

vllm:
  base_url: "http://localhost:8080/v1"
  api_key: "not-needed"

rag:
  embedding_model: "BAAI/bge-large-en-v1.5"
  reranker_model: "BAAI/bge-reranker-large"
  chunk_size: 700
  chunk_overlap: 120
  top_k: 4

memory:
  max_history: 10

cache:
  backend: "disk"  # memory, disk, redis
  ttl: 3600  # seconds
  path: "storage/cache"
