llm:
  provider: "vllm"  # options: vllm, ollama
  default_model: "meta-llama/Llama-3.1-8B-Instruct"
  temperature: 0.6
  max_tokens: 2048
  timeout: 30

vllm:
  base_url: "http://localhost:8080/v1"
  api_key: "not-needed"
  
ollama:
  base_url: "http://localhost:11434"
  model: "meta-llama/Llama-3.1-8B-Instruct"

rag:
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  chunk_size: 700
  chunk_overlap: 120
  top_k: 4

memory:
  max_history: 10

cache:
  backend: "disk"  # memory, disk, redis
  ttl: 3600  # seconds
  path: "storage/cache"
