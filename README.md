# ğŸ§  OmniCortex - Multi-Agent RAG Platform

**Version 2.0** | Modern AI chatbot platform with multi-agent support, RAG pipeline, and omnichannel deployment.

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸš€ Features

- **ğŸ¤– Multi-Agent System** - Create unlimited isolated AI agents with custom knowledge bases
- **ğŸ“š RAG Pipeline** - Upload PDFs, DOCX, TXT, CSV for agent-specific knowledge
- **ğŸ’» Local LLM** - Run Llama 3.1 via vLLM (zero API costs)
- **ğŸ™ï¸ Voice Chat** - Real-time audio with LiquidAI or ElevenLabs
- **ğŸ’¬ WhatsApp Integration** - Deploy agents via WhatsApp Business API
- **ğŸ“Š Analytics** - ClickHouse integration for usage tracking
- **ğŸ”„ Persistent Memory** - Conversation history per user/agent
- **âš¡ High Performance** - Handles 50+ concurrent agents

---

## ğŸ“‹ Quick Start

### Cloud Deployment (RunPod - Recommended) â˜ï¸

**5-minute setup with cost-effective GPU instances**

1. **Create RunPod Account**: [RunPod.io](https://runpod.io)
2. **Deploy Pod**: Select PyTorch template + RTX 4090 GPU
3. **Connect**: Use Web Terminal or SSH
4. **Run Script**:
```bash
git clone <your-repo-url> /workspace/OmniCortex
cd /workspace/OmniCortex
chmod +x scripts/deploy_runpod.sh
sudo ./scripts/deploy_runpod.sh
```
5. **Access**: Get URLs from RunPod dashboard (ports 8000, 8501)

**Cost**: ~$0.34/hr (~$245/month for 24/7)

See [RUNPOD.md](docs/RUNPOD.md) for detailed guide.

---

### Local Development ğŸ’»

### Prerequisites

- **Python 3.12+**
- **PostgreSQL 16+** with pgvector extension
- **NVIDIA GPU** (24GB+ VRAM recommended - RTX 4090, RTX 3090, A40, or A100)
- **uv** package manager
- **RunPod account** (for cloud deployment) or local GPU setup

### 1. Install uv

```bash
# Windows (PowerShell)
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# Linux/macOS
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Clone & Setup

```bash
git clone <your-repo-url>
cd OmniCortex

# Create virtual environment
uv venv --python 3.12

# Activate (Windows)
.venv\Scripts\activate

# Activate (Linux/macOS)
source .venv/bin/activate

# Install dependencies
uv pip install -e .
```

### 3. Configure Environment

```bash
# Copy example config
copy .env.example .env

# Edit .env with your settings
notepad .env
```

**Required settings:**
```env
DATABASE_URL=postgresql://postgres:YOUR_PASSWORD@localhost:5432/omnicortex
VLLM_BASE_URL=http://localhost:8080/v1
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
```

### 4. Setup Database

```bash
# Create database
psql -U postgres -c "CREATE DATABASE omnicortex;"

# Enable pgvector extension
psql -U postgres -d omnicortex -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

### 5. Start Services

**Terminal 1 - Start vLLM Server:**
```bash
docker run --gpus all -p 8080:8000 ^
  -v %USERPROFILE%\.cache\huggingface:/root/.cache/huggingface ^
  vllm/vllm-openai:latest ^
  --model meta-llama/Meta-Llama-3.1-8B-Instruct ^
  --max-model-len 8192
```

**Terminal 2 - Start API Server:**
```bash
uv run python api.py
```

**Terminal 3 - Start Next.js Admin Panel:**
```bash
cd admin
npm run dev
```

### 6. Access Application

- **Admin Panel**: http://localhost:3000
- **API Docs**: http://localhost:8000/docs
- **Metrics**: http://localhost:8000/metrics

---

## ğŸ“– Documentation

Comprehensive guides available in the `docs/` folder:

| Document | Description |
|----------|-------------|
| [PROJECT.md](docs/PROJECT.md) | Architecture & design decisions |
| [SETUP.md](docs/SETUP.md) | Detailed installation guide |
| [DEPLOYMENT_COMPARISON.md](docs/DEPLOYMENT_COMPARISON.md) | Compare deployment options |
| [RUNPOD.md](docs/RUNPOD.md) | RunPod GPU deployment (Recommended) |
| [DEPLOY.md](docs/DEPLOY.md) | General production deployment |
| [POSTGRESQL.md](docs/POSTGRESQL.md) | Database setup & configuration |
| [vLLM.md](docs/vLLM.md) | LLM server setup & tuning |
| [LLM.md](docs/LLM.md) | Model selection guide |
| [CLICKHOUSE.md](docs/CLICKHOUSE.md) | Analytics setup (optional) |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CLIENT LAYER                           â”‚
â”‚   [Next.js Admin]  [WhatsApp API]  [Voice/LiquidAI]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   APPLICATION LAYER                         â”‚
â”‚   [FastAPI :8000]  â†â†’  [vLLM Server :8080]                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CORE SERVICES                           â”‚
â”‚   [Agent Manager]  [Chat Service]  [RAG Pipeline]           â”‚
â”‚   [Document Processor]  [Vector Store]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DATA LAYER                            â”‚
â”‚   [PostgreSQL + pgvector]  [ClickHouse]  [File Storage]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Backend** | Python 3.12, FastAPI, SQLAlchemy |
| **UI** | Next.js + TypeScript + Tailwind CSS |
| **LLM** | vLLM + Llama 3.1-8B-Instruct |
| **Embeddings** | HuggingFace all-MiniLM-L6-v2 |
| **Database** | PostgreSQL 16 + pgvector |
| **Analytics** | ClickHouse (optional) |
| **Voice** | LiquidAI / ElevenLabs |
| **Orchestration** | LangChain, LangGraph, CrewAI |
| **Package Manager** | uv (Astral) |

---

## ğŸ“Š Performance

Tested on RTX 4090 (24GB VRAM):

| Metric | Target | Achieved |
|--------|--------|----------|
| Concurrent Agents | 50+ | âœ… 80 |
| Response Latency | <3s | âœ… 1-2s |
| Throughput | 2000+ tok/s | âœ… 5200 tok/s |
| API Cost | $0 | âœ… Local LLM |
| Hosting Cost | - | âœ… $0.34/hr (RunPod) |

---

## ğŸ”§ Configuration

### Port Configuration

| Service | Port | Description |
|---------|------|-------------|
| vLLM Server | 8080 | LLM inference engine |
| FastAPI | 8000 | REST API backend |
| Next.js Admin | 3000 | Web Admin Panel |
| PostgreSQL | 5432 | Database |
| ClickHouse | 8123 | Analytics (optional) |

### Environment Variables

See `.env.example` for all available configuration options.

**Core Settings:**
- `DATABASE_URL` - PostgreSQL connection string
- `VLLM_BASE_URL` - vLLM server endpoint
- `VLLM_MODEL` - Model name/path
- `EMBEDDING_MODEL` - HuggingFace embedding model

**Optional:**
- `WHATSAPP_ACCESS_TOKEN` - Meta Business API token
- `WHATSAPP_PHONE_ID` - WhatsApp phone number ID
- `CLICKHOUSE_HOST` - Analytics database host

---

## ğŸ¯ Use Cases

- **Customer Support** - Deploy AI agents with company knowledge
- **Internal Knowledge Base** - Query documents via chat
- **Multi-tenant SaaS** - Isolated agents per customer
- **Voice Assistants** - Real-time audio conversations
- **WhatsApp Bots** - Automated messaging with RAG

---

## ğŸ› Troubleshooting

### "Connection refused" (Database)
```bash
# Windows: Start PostgreSQL service
services.msc â†’ postgresql-x64-16 â†’ Start

# Linux
sudo systemctl start postgresql
```

### "CUDA out of memory"
Reduce vLLM batch size:
```bash
--max-num-seqs 64 --gpu-memory-utilization 0.85
```

### "No module named 'core'"
```bash
uv pip install -e .
```

### vLLM not responding
Check if server is running:
```bash
curl http://localhost:8080/health
```

---

## ğŸ“ Project Structure

```
OmniCortex/
â”œâ”€â”€ core/                   # Core modules
â”‚   â”œâ”€â”€ agent_manager.py    # Agent CRUD
â”‚   â”œâ”€â”€ chat_service.py     # RAG orchestration
â”‚   â”œâ”€â”€ llm.py              # LLM integration
â”‚   â”œâ”€â”€ database.py         # SQLAlchemy models
â”‚   â”œâ”€â”€ processing/         # Document processing
â”‚   â”œâ”€â”€ rag/                # Vector store & embeddings
â”‚   â””â”€â”€ voice/              # Voice processing
â”œâ”€â”€ config/                 # YAML configurations
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ scripts/                # Deployment scripts
â”œâ”€â”€ tests/                  # Test suite
â”œâ”€â”€ api.py                  # FastAPI backend
â”œâ”€â”€ admin/                 # Next.js Admin Panel
â””â”€â”€ pyproject.toml          # Dependencies
```

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“„ License

MIT License - see LICENSE file for details

---

## ğŸ™ Acknowledgments

- **Meta** - Llama 3.1 models
- **vLLM Team** - High-performance inference
- **LangChain** - LLM orchestration framework
- **PostgreSQL** - Reliable database
- **Next.js** - Modern React framework for admin panel

---

## ğŸ“ Support

- **Documentation**: See `docs/` folder
- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions

---

**Built with â¤ï¸ for the AI community**
